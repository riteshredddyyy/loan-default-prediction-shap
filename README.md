


# ğŸ“˜ Loan Default Prediction with SHAP Explainability

Most models can tell you **what** they predict. But few can explain **why**.  
In this project, we built a real-world **loan approval classifier** â€” and made it transparent using SHAP.

---

## ğŸ“‚ Dataset Overview

We used the [Bank Personal Loan Dataset](https://www.kaggle.com/datasets/yasserh/loan-default-dataset) that simulates banking data across demographics, accounts, and approvals.

| Column Name        | Description                               |
|--------------------|-------------------------------------------|
| `Age`, `Income`    | Demographic info                          |
| `Mortgage`, `CCAvg`| Financial behavior                        |
| `Education`, `Family` | Profile attributes                    |
| `CD Account`, `Online`, `CreditCard` | Account types            |
| `Personal Loan`    | âœ… **Target**: 1 = Approved, 0 = Rejected |

---

## ğŸ§  Why This Project?

Our previous models (like clustering or regression) were excellent for understanding customer behavior.

This time, we wanted to:
- Make **binary decisions** (approve or reject loan)
- Learn how to **explain model predictions**
- Use this project as a **launchpad into trustworthy AI**

---

## ğŸ§ª Step-by-Step Process

### 1ï¸âƒ£ Data Exploration + Cleaning

We started by understanding feature distributions and checking class imbalance:

<p align="center">
  <img src="./assets/class_imbalance.png" width="420"/>
</p>

> We had class imbalance (fewer approvals), but not extreme.  
> So we opted for **class weights** instead of oversampling.

---

### 2ï¸âƒ£ Feature Engineering + Preprocessing

- No missing values  
- Encoded categorical features like `Education`, `Family`  
- Scaled all numerical features for tree models  
- Train-test split: 80/20  

---

### 3ï¸âƒ£ Model Training & Evaluation

We used two models:
- âœ… **Logistic Regression** (baseline)
- âœ… **XGBoost** (advanced)

<p align="center">
  <img src="./assets/xgb_confusion_matrix.png" width="420"/>
</p>

> XGBoost outperformed Logistic Regression across the board.

### ğŸ“ˆ Final Performance (XGBoost)

| Metric        | Score   |
|---------------|---------|
| Accuracy      | 99%     |
| ROC AUC       | 0.999   |
| Precision (1) | 91%     |
| Recall (1)    | 98%     |

âœ… No underfitting or overfitting.  
âœ… Model generalized well on unseen customers.

---

## ğŸ” Model Explainability (SHAP)

Understanding **why** a model made a decision is as important as the decision itself â€” especially in finance.

We used **SHAP (SHapley Additive exPlanations)** to break down individual predictions.

### ğŸ“Š Global Feature Importance

<p align="center">
  <img src="./assets/shap_summary_bar.png" width="500"/>
</p>

> Features like **Income**, **CD Account**, and **Credit Card Usage** strongly influenced decisions.

---

### ğŸ” Local Explanation for a Single Prediction

We asked the model to explain **why it approved one specific customer**.

<p align="center">
  <img src="./assets/shap_force_sample1.png" width="500"/>
</p>

### ğŸ¤– Model Reasoning (for this approval)

- ğŸ’° **Income = 2.39** â†’ Strongly increased approval likelihood  
- ğŸ’¾ **CD Account = 1** â†’ Indicates strong financial health  
- ğŸ’³ **CreditCard = 1** â†’ Slight credibility boost  
- ğŸ  **Mortgage = 0.01** â†’ Very low debt = less risky  
- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ **Family = 4** â†’ Suggests responsibility  

> âœ… This was **not a black-box**.  
> SHAP helped us understand the â€œwhyâ€ behind the prediction â€” a critical step for trust in AI.

---

## ğŸ§­ Summary of Steps

Here's the journey in 7 clear steps:

1. ğŸ“¥ Load + explore the dataset (class imbalance, correlation)  
2. ğŸ§¼ Clean and encode the features  
3. ğŸ”€ Train/test split (with class weights for imbalance)  
4. âš™ï¸ Train Logistic Regression + XGBoost  
5. ğŸ§¾ Evaluate models (confusion matrix, ROC AUC)  
6. ğŸ” Explain global & local predictions with SHAP  
7. ğŸ’¡ Humanize decisions for business understanding  

---

## ğŸ§  What I Learned

- Explainability (SHAP) is *not optional* for real-world ML  
- Class imbalance needs strategy, not just oversampling  
- XGBoost isnâ€™t just powerful â€” itâ€™s interpretable too  
- **Performance metrics** must be balanced with **trust**  

---

## ğŸ“¦ Folder Structure

```
loan-default-prediction/
â”œâ”€â”€ assets/                       # All saved plots (confusion matrix, SHAP, etc.)
â”œâ”€â”€ notebook.ipynb                # Jupyter Notebook (step-by-step)
â”œâ”€â”€ data/                         # Raw dataset (xlsx or CSV)
â””â”€â”€ README.md                     # Youâ€™re here!
```

---

## ğŸš€ Whatâ€™s Next?

This project now acts as the **classification bridge** between:
- ğŸ‘ˆ Previous clustering/segmentation projects  
- ğŸ‘‰ Upcoming projects in **fairness**, **risk modeling**, and **trustworthy ML**

---

> Built as part of my journey to not just make AI smarter â€” but make it understandable.